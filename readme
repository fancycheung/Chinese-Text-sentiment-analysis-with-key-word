
Requirement:
spider python 3.6, Keras 2.2.0.
Make sure the Keras.backend from Theano, the default is Tensorflow, can be switched manually. (See Keras Document: https://keras.io/backend/)

Acknowlegement:

1. In this project, pertained wordvectors which apply Chinese Wikipedia corpus is used to assign the word-embedding matrix. [reference: https://github.com/Embedding/Chinese-Word-Vectors] "Analogical Reasoning on Chinese Morphological and Semantic Relations".

2. This project follows Yang et al. [reference:https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] "Hierarchical Attention Networks for Document Classification".


Process:

1. Initialization
(1) Reading Data/ Clean Text/ Tokenize
Some issues:
a.The data is unbalanced between neg and pos, the ratio is around 1:1.5;
b.The missing values which labeled as "na" in "neg" are almost 1/3 of the whole data set, and not missing value in "pos" class, thus, the distribution of missing value is not random. Further investigated is needed.
If the data collection is reliable, one possible solution to treat the nonrandom missing value is to imputation.

For time reason, these issues are ignored in this project, missing value is deleted roughly in this project.


(2) Build Embedding Matrix;
(3) Transform reviews to word sequence;
 
  Investigate the length of Review Sequence after tokenized.
  By quantile analysis, 70% reviews sequence are less than 6.
  To avoid the data are too sparse, choose the maximum sequence length as 6.


    For basic dimension reduction, two ways can be applied to restrict the word into dictionary,
    one is to restrict the word which is appeared less than some frequency, 
    the other is to set a maximum length of the dictionary.
   e.g. #word2index = word2index[word2index >= 5]  
        #word2index=word2index[:max(len(word2index) , maxvocab)]


2: Model


"""
 (1) make sure use theano based BACKEND
"""

"""
below are three methods to solve it
"""

os.environ['KERAS_BACKEND'] = 'theano'


def set_keras_backend(backend):

    if K.backend() != backend:
        os.environ['KERAS_BACKEND'] = backend
        importlib.reload(K)
        assert K.backend() == backend

set_keras_backend("theano")


def dot_product(x, kernel):
    """
    Wrapper for dot product operation, in order to be compatible with both
    Theano and Tensorflow
    Args:
        x (): input
        kernel (): weights
    Returns:
    """
    if K.backend() == 'tensorflow':
        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)
    else:
        return K.dot(x, kernel)

(2)Attention Mechanism
In order to capture the informations from whole networks, serveral Attention mechanisms are studied in deep learning.
Below are two Attention Layer customized in Keras.
The work of Raffel et al. [https://arxiv.org/abs/1512.08756] implements an Attention mechanism for temporal data.(Attention)
The work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] "Hierarchical Attention Networks for Document Classification" use a context vector to assist the attention. (AttentionHAN)
source code:https://gist.github.com/cbaziotis

(3) Build Sequential Model      
       sequence_input = Input(shape=(maxlen,), dtype='int32')
       embedded_sequences = Embedding(len(word2index.index) + 1, EDIM,
                            weights=[embedding_matrix],mask_zero=False,
                            input_length=maxlen,trainable=False)(sequence_input)
       gru = Bidirectional(GRU(100,return_sequences=True))(embedded_sequences)
       Attention = Attention()(gru)
       dense = Dense(1, activation='softmax')(Attention)
       model = Model(sequence_input, dense)
       model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])
       
       model.summary()
       


3. Train

# Get Training Samples
model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=256)
       
 
4. Predict:
To compute the attention weights, need to get GRU and Attention layers out, then calculate attention weights by formula:
np.exp(np.tanh(np.dot(out[i], out2[i])))/np.sum(np.exp(np.tanh(np.dot(out[i], out2[i])))).


5. Final Thought:  apply dropout and mask might achive better result. “Filter pattern” can be customized in first stage.(PreDoc)


Experiments:

(1)Toy Example:

Attached includes a pickle file TOYData.pickle prepared with a toy sample, 2000 samples, including clean data, embedding matrix.
By loading TOYData in ‘A Toy Example.py’, clean data, embedding matrix will be ready to process the model built-up, training, and prediction.
Following are the TOYmodel summary, a simple training process, and prediction with key words result:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_21 (InputLayer)        (None, 6)                 0         
_________________________________________________________________
embedding_18 (Embedding)     (None, 6, 300)            809400    
_________________________________________________________________
bidirectional_14 (Bidirectio (None, 6, 128)            140160    
_________________________________________________________________
attention_19 (Attention)     (None, 128)               134       
_________________________________________________________________
dense_10 (Dense)             (None, 2)                 258       
=================================================================
Total params: 949,952
Trainable params: 140,552
Non-trainable params: 809,400
_________________________________________________________________
None
Train on 1400 samples, validate on 600 samples
Epoch 1/1
1400/1400 [==============================] - 474s 339ms/step - loss: 0.6315 - acc: 0.6779 - val_loss: 0.5017 - val_acc: 0.8117

(0 : neg; 1: pos)

   0         1
0  1      [, ]
1  1    [品牌, ]
2  0  [色彩, 鲜明]
3  0   [电池, 不]
4  1      [, ]
5  0  [暂时, 没有]
6  1  [不错, 快捷]
7  1   [很, 不错]
8  0   [长, 还要]
9  0   [感觉, 元]



(2)MYmodel:
MYData.pickle prepared with all the sample data, corresponding MYmodel trained with 30000 training sample, the training weights has been saved in  MYmodel_weights.h5, can be loaded in the model and continue training.
The acc rate and val rate of MYmodel achieve around 0.9.

MYmodel=BuildModel(maxlen=6, EDIM=300, nunits=64, embedding_matrix=embedding_matrix)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_23 (InputLayer)        (None, 6)                 0         
_________________________________________________________________
embedding_20 (Embedding)     (None, 6, 300)            13199700  
_________________________________________________________________
bidirectional_15 (Bidirectio (None, 6, 128)            140160    
_________________________________________________________________
attention_20 (Attention)     (None, 128)               134       
_________________________________________________________________
dense_11 (Dense)             (None, 2)                 258       
=================================================================
Total params: 13,340,252
Trainable params: 140,552
Non-trainable params: 13,199,700
_________________________________________________________________
None


MYfit=MYmodel.fit(x_train[-20000:], y_train[-20000:], 
           batch_size=256, epochs=10, 
           validation_data=(x_val[-5000:], y_val[-5000:]))
Train on 20000 samples, validate on 5000 samples
Epoch 1/10
20000/20000 [==============================] - 3753s 188ms/step - loss: 0.4509 - acc: 0.7958 - val_loss: 0.4315 - val_acc: 0.8054
Epoch 2/10
20000/20000 [==============================] - 3529s 176ms/step - loss: 0.3557 - acc: 0.8488 - val_loss: 0.3419 - val_acc: 0.8582
Epoch 3/10
20000/20000 [==============================] - 3431s 172ms/step - loss: 0.3317 - acc: 0.8630 - val_loss: 0.3586 - val_acc: 0.8498
Epoch 4/10
20000/20000 [==============================] - 3434s 172ms/step - loss: 0.3161 - acc: 0.8669 - val_loss: 0.3143 - val_acc: 0.8664
Epoch 5/10
20000/20000 [==============================] - 3433s 172ms/step - loss: 0.3011 - acc: 0.8729 - val_loss: 0.3466 - val_acc: 0.8408
Epoch 6/10
20000/20000 [==============================] - 3439s 172ms/step - loss: 0.2903 - acc: 0.8782 - val_loss: 0.3445 - val_acc: 0.8494
Epoch 7/10
20000/20000 [==============================] - 3446s 172ms/step - loss: 0.2785 - acc: 0.8837 - val_loss: 0.3096 - val_acc: 0.8664
Epoch 8/10
20000/20000 [==============================] - 3461s 173ms/step - loss: 0.2680 - acc: 0.8900 - val_loss: 0.2980 - val_acc: 0.8744
Epoch 9/10
20000/20000 [==============================] - 3444s 172ms/step - loss: 0.2560 - acc: 0.8945 - val_loss: 0.3458 - val_acc: 0.8522
Epoch 10/10
20000/20000 [==============================] - 3431s 172ms/step - loss: 0.2469 - acc: 0.8987 - val_loss: 0.3058 - val_acc: 0.8756


testFile='testFile.txt'
#clean raw testFile
x_test = PreTest(testFile,maxlen=6)
#prediction with keywords
pred = PredKey(x_test,MYmodel,kw=2)
print(pred)
x_test ready
    0           1
0   1      [很, 好]
1   1      [描述, ]
2   1  [随身携带, 携带]
3   0     [王, 过级]
4   1    [非常, 坚固]
5   1      [好用, ]
6   1      [描述, ]
7   1      [, 简单]
8   1      [快, 得]
9   0      [还, 没]
10  1    [可以, 喜欢]
11  1  [价格便宜, 便宜]
12  1      [描述, ]
13  1    [放心, 不错]
14  1        [, ]
15  0     [具体, 不]
16  1    [可以, 满意]
17  1     [挺, 不错]
18  1        [, ]
19  1      [描述, ]
20  1      [描述, ]
21  1      [给, 很]
22  0     [體積, 較]
23  1      [好, 快]
24  1       [好, ]
25  1   [第五次, 原装]
26  1     [买, 不错]
27  1      [又, 快]
28  0        [, ]
29  1  [价格便宜, 小巧]
.. ..         ...
70  0      [瑕疵, ]
71  0        [, ]
72  0        [, ]
73  0      [略有, ]
74  0     [没, 按键]
75  0        [, ]
76  0    [发现, 没有]
77  0        [, ]
78  0    [时间, 不能]
79  0   [吸力, 不经用]
80  0        [, ]
81  0        [, ]
82  0     [声音, 没]
83  0        [, ]
84  0      [很难, ]
85  0    [发现, 没有]
86  0    [态度, 不是]
87  1        [, ]
88  0        [, ]
89  0     [量, 不好]
90  0    [自己, 不是]
91  1        [, ]
92  0        [, ]
93  0        [, ]
94  0      [有点, ]
95  0      [屏幕, ]
96  0        [, ]
97  1       [吧, ]
98  0        [, ]
99  0        [, ]

[100 rows x 2 columns]
